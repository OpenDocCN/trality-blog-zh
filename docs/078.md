# 强化学习的要点(以及我们为什么应该关注它)

> 原文:[https://www . trality . com/blog/reinforcement-learning-a-dynamic-way-of-thinking/](https://www.trality.com/blog/reinforcement-learning-a-dynamic-way-of-thinking/)

“学习”一词在我们的日常生活中变得越来越普遍。公平而有把握地说，这一趋势的很大一部分可以归因于人工智能背景下机器学习技术和方法的显著崛起，人工智能在广泛的定量科学中占据了突出的地位，包括计算机视觉(CV)、自动驾驶和自然语言处理(NLP)，仅举几例。

虽然一些机器学习问题可以以“静态”的方式有效地解决(即，学习过程一次性分析所有可用的数据)，但许多其他问题更自然地以“动态”的方式构建(即，学习过程被分成连续的步骤，其中每个步骤都与算法或算法的用户可以在给定问题的范围内采取的可解释的、有意义的动作相关联)。这些动态结构化的问题构成了通常称为“强化学习”的机器学习子领域。

从概念的角度来看，强化学习令人着迷。但它也允许我们描述我们在 Trality 最感兴趣的内在动态对象，即金融市场，这就是为什么我们开始了关于这个主题的博客系列。在第一部分中，我们将探讨强化学习的基本思想，然后解释它与 Trality 核心业务的潜在联系。从写得漂亮的报告中汲取灵感。CSS-18up 66p { color:# 00b8e 6；字体粗细:正常；光标:指针；左:0px 位置:相对；-webkit-transition:全部 500ms 转场:全部 500ms-webkit-text-decoration:无；文字-装饰:无；} [“金融市场中的强化学习——一项调查”作者 T. G. Fischer](https://ideas.repec.org/p/zbw/iwqwdp/122018.html) ，对于这篇文章的大部分内容，我们甚至不需要参考与金融相关的例子。事实上，我们开始的时候...基本的。

**一个基本的例子**

目前，我们将把所有的考虑因素与下面的例子联系起来。

基本示例:假设你被一个主持人邀请去尝试一个新的多步骤游戏，这个游戏本质上涉及到在每一步做出战略决策，目标是在游戏结束时最大化某种奖励。

你很难找到一个更通用的描述，但是，就目前而言，这基本上就是你需要知道的全部了。不要担心，我们会在任何需要的时候提供更多这样游戏的具体实例。

根据主人决定如何安排，你可能会发现自己处于两种不同但同样真实的场景中。

*场景 1。*由于你以前从未玩过，主持人决定让你和一位有经验的玩家配对，这位玩家以前玩过几次。这个有经验的玩家能够用一组特征(或“状态”)概括整个游戏的环境，并且还能够预测状态可能如何从一步到另一步变化。你的参与仅限于根据你从有经验的玩家那里得到的预测来决定在游戏的每一步采取什么“行动”。

*场景二。主持人希望你从错误中吸取教训，因此强迫你自己玩。因为你没有经验，所以给你几轮非正式的游戏让你练习，在每一步你都要考虑游戏的当前状态并评估你的行动。你跟踪你的评估，并在游戏的每一轮中不断更新它们。*

**机器学习**

在场景 1 中，有经验的玩家在多年的过程中“学习”到游戏的某些状态可能会导致一些其他的特定状态。用数学术语来说，有经验的玩家见过很多对(x_i，y_i)，x_i 是一种状态，y_i 是 x_i 之后的观察状态。这种丰富的知识允许有经验的玩家做出预测，如果面对新的配置 x。

在许多情况下，这种学习任务也可以由机器有效地执行(简而言之，这就是机器学习)，当然，前提是所述机器可以找出适当的“映射”,将数据 x 忠实地链接到观察值 y。更准确地说:

> *机器学习涉及让机器选择最精确的映射，在非常大(非常大！)可能映射的集合。*

人类提供的输入通常限于:

*   数据/观察结果
*   定义所有可探索的映射
*   评估每个地图在预测任务中表现的方式

机器做所有其他事情，也就是说，它使用大量的计算能力来探索指定的映射集，并最终挑选出合适的映射。现代计算机的高水平计算能力不仅使这种探索成为可能，而且整个方法也是可行的。

强化学习:浅尝辄止

在场景 1 中，有经验和无经验玩家的角色是“分离”的。换句话说，没有经验的玩家接受有经验的玩家(唯一一个经过训练的玩家)的预测，并据此行动。这种双重方法有一些限制:

1.  没有经验的玩家无法评估接收到的预测是否准确。此外，即使准确，这些预测也可能是针对他/她不感兴趣的任务而校准的。假设正在讨论的游戏是[冒险](https://en.wikipedia.org/wiki/Risk_(game))，例如，如果有经验的玩家只接受了征服大多数领土的任务训练，而没有经验的玩家却被指示去摧毁蓝军，那会怎么样？这种不匹配可能会导致一个次优的策略，而不是为没有经验的玩家量身定制的。
2.  在每一步，没有经验的玩家很可能只得到一些概括的特征，这些特征反映了有经验的玩家的预测。公平地说，这在许多情况下都是有意义的:如果游戏只是简单地“决定第二天穿什么”，你肯定不会希望在电视天气预报中获得所有可用气象数据的全部——仅仅是具体的特征，例如降雨的可能性和温度范围。然而，在其他情况下，这也可能阻止没有经验的玩家利用对游戏状态的全面和彻底的分析(如果他/她需要的话)。
3.  有经验的玩家没有被训练过的任何约束都不会被考虑。例如，如果采取某些行动需要支付某种费用，而有经验的玩家并不知道，那该怎么办？这将最终影响无经验玩家的最终奖励。

为了解决上述问题，已经提出了机器学习的进一步改进，称为“强化学习”。简而言之，这种方法“融合”了有经验和无经验玩家的角色。最终的玩家需要在游戏中接受训练，探索所有可以做出的动作的效果，最重要的是，根据他/她最终感兴趣的相同标准来判断它们，同时也考虑所有相关的限制。换句话说，训练结合了基于奖励函数的行为选择的状态预测*和*的同时改进，该奖励函数忠实地总结了玩家的收益。这种情况大致就是我们在场景 2 中描述的情况。

**术语总结**

我们现在已经熟悉了强化学习的基础知识，并且已经了解了一些基本术语，我们将在这里再次重申:

*   *状态*:从游戏当前的环境配置可以推断出的信息。
*   *动作*:游戏每一步可以执行的可能招式之一(即与游戏当前环境配置的可能交互之一)。
*   *奖励*:在某个游戏状态下采取某个动作所获得的收益。

**强化学习:挑选自己喜欢的风格**

到目前为止，我们已经将强化学习描述为这样一个过程，该过程结合了状态预测的同时改进和基于为玩家需求定制的奖励函数的动作选择。在这个总的范例中，还有更多分类要做。事实上，强化学习被进一步分成三种不同的情况，我们可以针对下面描述的同一个游戏来解释。

在这个游戏中，玩家的目标是在到达第一个死胡同之前最大化他/她在迷宫中游荡的时间。因此，玩家必须决定在迷宫的每个十字路口做什么，并且可能只依赖两种工具:

*   *所有迷宫十字路口的地图*，标明在每个十字路口采取任何给定方向(行动)可以获得的奖励。信息可能是不完整的(例如，由于玩家没有获得任何信息，地图在游戏开始时是空白的)。
*   *他/她的嗅觉*，直接告诉玩家在每个十字路口应该采取的正确行动(*不需要*评估所有可能的行动)。就像地图一样，这种嗅觉也需要一些微调。

强化学习技术大致分为三大类:

1.  基于评论家的方法:玩家除了地图什么都不用，并在每一个转折处更新它的(最初是空白的)内容。每一步都要评估每一个允许动作的效果。
2.  *基于演员的*方法:玩家只使用他/她的嗅觉，即学习直接将状态映射到动作*，而不需要*审查它们。
3.  演员兼评论家的方法:玩家使用两种工具。这样，两种工具(“直觉/直觉”嗅觉和“合理”使用地图)都是互利的，就像两个性格不同的人学会互补，并作为一对工作，提高他们的个人表现一样。

<figure class="kg-card kg-image-card kg-width-full">![](../Images/915fe9ecab99a19a94f603b655651f4e.png)<picture><source type="image/webp" data-srcset="/static/1331f43791dad1495065feacd6e5c7dd/b32c3/EZ9ynXFELQxguPW9sT4dON4bgR1hGXUgfxLvVRy07ZfBjGFrENstikJTM1U9Df4fL63NeMdBc2TC5hVQqDSY6uUdoHcTfeJry1S5jTCRP5B1_ms4syZMHLgi1qTwmyHiGuJJfoK4%3Ds1600.webp 128w,/static/1331f43791dad1495065feacd6e5c7dd/1215c/EZ9ynXFELQxguPW9sT4dON4bgR1hGXUgfxLvVRy07ZfBjGFrENstikJTM1U9Df4fL63NeMdBc2TC5hVQqDSY6uUdoHcTfeJry1S5jTCRP5B1_ms4syZMHLgi1qTwmyHiGuJJfoK4%3Ds1600.webp 256w,/static/1331f43791dad1495065feacd6e5c7dd/c2cba/EZ9ynXFELQxguPW9sT4dON4bgR1hGXUgfxLvVRy07ZfBjGFrENstikJTM1U9Df4fL63NeMdBc2TC5hVQqDSY6uUdoHcTfeJry1S5jTCRP5B1_ms4syZMHLgi1qTwmyHiGuJJfoK4%3Ds1600.webp 512w" sizes="(min-width: 512px) 512px, 100vw">![](../Images/abd9f444f35338cea42214b10fb8bb4b.png)</picture>

<noscript><picture><source type="image/webp" srcset="/static/1331f43791dad1495065feacd6e5c7dd/b32c3/EZ9ynXFELQxguPW9sT4dON4bgR1hGXUgfxLvVRy07ZfBjGFrENstikJTM1U9Df4fL63NeMdBc2TC5hVQqDSY6uUdoHcTfeJry1S5jTCRP5B1_ms4syZMHLgi1qTwmyHiGuJJfoK4%3Ds1600.webp 128w,/static/1331f43791dad1495065feacd6e5c7dd/1215c/EZ9ynXFELQxguPW9sT4dON4bgR1hGXUgfxLvVRy07ZfBjGFrENstikJTM1U9Df4fL63NeMdBc2TC5hVQqDSY6uUdoHcTfeJry1S5jTCRP5B1_ms4syZMHLgi1qTwmyHiGuJJfoK4%3Ds1600.webp 256w,/static/1331f43791dad1495065feacd6e5c7dd/c2cba/EZ9ynXFELQxguPW9sT4dON4bgR1hGXUgfxLvVRy07ZfBjGFrENstikJTM1U9Df4fL63NeMdBc2TC5hVQqDSY6uUdoHcTfeJry1S5jTCRP5B1_ms4syZMHLgi1qTwmyHiGuJJfoK4%3Ds1600.webp 512w" sizes="(min-width: 512px) 512px, 100vw"/><img data-gatsby-image-ssr="" data-main-image="" style="opacity:0" sizes="(min-width: 512px) 512px, 100vw" decoding="async" loading="lazy" src="../Images/abd9f444f35338cea42214b10fb8bb4b.png" srcset="/static/1331f43791dad1495065feacd6e5c7dd/e481c/EZ9ynXFELQxguPW9sT4dON4bgR1hGXUgfxLvVRy07ZfBjGFrENstikJTM1U9Df4fL63NeMdBc2TC5hVQqDSY6uUdoHcTfeJry1S5jTCRP5B1_ms4syZMHLgi1qTwmyHiGuJJfoK4%3Ds1600.jpg 128w,/static/1331f43791dad1495065feacd6e5c7dd/7e250/EZ9ynXFELQxguPW9sT4dON4bgR1hGXUgfxLvVRy07ZfBjGFrENstikJTM1U9Df4fL63NeMdBc2TC5hVQqDSY6uUdoHcTfeJry1S5jTCRP5B1_ms4syZMHLgi1qTwmyHiGuJJfoK4%3Ds1600.jpg 256w,/static/1331f43791dad1495065feacd6e5c7dd/1c89c/EZ9ynXFELQxguPW9sT4dON4bgR1hGXUgfxLvVRy07ZfBjGFrENstikJTM1U9Df4fL63NeMdBc2TC5hVQqDSY6uUdoHcTfeJry1S5jTCRP5B1_ms4syZMHLgi1qTwmyHiGuJJfoK4%3Ds1600.jpg 512w" alt="" data-original-src="https://www.trality.com/static/1331f43791dad1495065feacd6e5c7dd/1c89c/EZ9ynXFELQxguPW9sT4dON4bgR1hGXUgfxLvVRy07ZfBjGFrENstikJTM1U9Df4fL63NeMdBc2TC5hVQqDSY6uUdoHcTfeJry1S5jTCRP5B1_ms4syZMHLgi1qTwmyHiGuJJfoK4%3Ds1600.jpg"/></picture></noscript>

</figure>

总的来说，强化学习可以这样总结:

> *强化学习是机器学习的一个子领域。除了上面第 3 节中描述的基本要求之外，还需要训练机器描述与给定环境的动态交互。具体来说，一方面，机器的内部算法需要用连续的、可解释的动作来表述。另一方面，它们需要由奖励机制来驱动，奖励机制是由用户指定的。*

**为什么我们在 Trality 关注强化学习**

正如所承诺的，在我们的讨论中，我们已经多次证实了我们的“基本例子”,以说明一些核心思想。

冒险和在想象的迷宫中寻找出路听起来绝对像是值得偶尔进行的娱乐活动。然而，正如我们在这篇文章的开头所暗示的，我们在 Trality 的最终兴趣是玩一种不同的游戏(与这篇文章的语言保持一致)。

在其核心，Trality 已经提出了一个有远见的平台，为不断增长的用户群带来了几个用于创建、设计和整合自动交易机器人(即，用于在金融市场上买卖股票的预定义策略)的最先进工具。更具体地说，Trality 正在走向民主化，使每个人都可以进行交易(不仅仅是那些在业内的人)，它正在通过让用户(或“机器人创建者”)负责在直接打包的功能和最适合他/她的高技能编码功能之间取得平衡来实现这一目标。

最终，由我们的用户创建的任何给定的交易机器人都将不断面对金融环境的当前“状态”(例如，股票价格、非价格金融指标、情绪数据——你能想到的)。因此，机器人将需要执行一些“动作”(如购买/出售股票，或重塑用户的投资组合)。正如你可能已经猜到的，“状态”和“行动”反映了这样一个事实，即这个财务场景确实可以用强化学习的语言来描述，我们在上面已经介绍过了。

在我们努力提供更好、更具包容性的用户体验时，我们坚信不断提高的可解释性是我们所有产品都将具备的一个关键特性。强化学习范式提供的直观性和可解释性使其成为 Trality 等领先金融科技平台的相关组成部分。因此，我们打算在未来的产品中加入强化学习的元素，并用它来为我们的机器人创造者打造更好的工具。

* * *

好吧，我现在知道要点了。但是，从代码的角度来看，强化学习是什么样的？使其成为可能的数学基础是什么？请收听这个关于强化学习的博客系列的第二集，我们会深入探讨。